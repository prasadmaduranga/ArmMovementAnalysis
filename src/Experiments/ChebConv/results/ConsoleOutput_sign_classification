/usr/local/bin/python3.9 "/Users/prasadmaduranga/higher studies/research/Stroke research/Projects/Staying connected Project/My Projects/ArmMovementAnalysis/src/Experiments/ChebConv/code/ChebConv.py"
Epoch: 0, train_loss: 2.0723891258239746, valid_loss: 2.0668320655822754, time: [54.63], best model: 1
Epoch: 1, train_loss: 2.064321756362915, valid_loss: 2.0582082271575928, time: [44.21], best model: 1
Epoch: 2, train_loss: 2.057013750076294, valid_loss: 2.049349784851074, time: [43.75], best model: 1
Epoch: 3, train_loss: 2.0495126247406006, valid_loss: 2.0407557487487793, time: [44.79], best model: 1
Epoch: 4, train_loss: 2.0417370796203613, valid_loss: 2.0300843715667725, time: [44.22], best model: 1
Epoch: 5, train_loss: 2.033494234085083, valid_loss: 2.0198781490325928, time: [44.54], best model: 1
Epoch: 6, train_loss: 2.0255627632141113, valid_loss: 2.0111868381500244, time: [44.39], best model: 1
Epoch: 7, train_loss: 2.0158421993255615, valid_loss: 2.0012998580932617, time: [43.56], best model: 1
Epoch: 8, train_loss: 2.006734609603882, valid_loss: 1.989031195640564, time: [43.9], best model: 1
Epoch: 9, train_loss: 1.9972224235534668, valid_loss: 1.9810694456100464, time: [44.02], best model: 1
Epoch: 10, train_loss: 1.9876374006271362, valid_loss: 1.9626110792160034, time: [42.85], best model: 1
Epoch: 11, train_loss: 1.9775952100753784, valid_loss: 1.9545353651046753, time: [44.69], best model: 1
Epoch: 12, train_loss: 1.9675322771072388, valid_loss: 1.9427800178527832, time: [43.57], best model: 1
Epoch: 13, train_loss: 1.9575356245040894, valid_loss: 1.9274054765701294, time: [42.29], best model: 1
Epoch: 14, train_loss: 1.9488853216171265, valid_loss: 1.9211684465408325, time: [43.69], best model: 1
Epoch: 15, train_loss: 1.9386361837387085, valid_loss: 1.9105793237686157, time: [42.75], best model: 1
Epoch: 16, train_loss: 1.928852915763855, valid_loss: 1.9055627584457397, time: [43.94], best model: 1
Epoch: 17, train_loss: 1.9187424182891846, valid_loss: 1.8946934938430786, time: [42.95], best model: 1
Epoch: 18, train_loss: 1.9102891683578491, valid_loss: 1.8862671852111816, time: [43.98], best model: 1
Epoch: 19, train_loss: 1.9003331661224365, valid_loss: 1.8716801404953003, time: [52.91], best model: 1
Epoch: 20, train_loss: 1.891826868057251, valid_loss: 1.8623889684677124, time: [2043.82], best model: 1
Epoch: 21, train_loss: 1.8839035034179688, valid_loss: 1.8513569831848145, time: [135.33], best model: 1
Epoch: 22, train_loss: 1.8748444318771362, valid_loss: 1.843309760093689, time: [2016.41], best model: 1
Epoch: 23, train_loss: 1.866154670715332, valid_loss: 1.8416764736175537, time: [7318.27], best model: 1
Epoch: 24, train_loss: 1.858477234840393, valid_loss: 1.835119366645813, time: [137.72], best model: 1
Epoch: 25, train_loss: 1.8502050638198853, valid_loss: 1.8300657272338867, time: [140.26], best model: 1
Epoch: 26, train_loss: 1.8429580926895142, valid_loss: 1.8145643472671509, time: [137.74], best model: 1
Epoch: 27, train_loss: 1.8363749980926514, valid_loss: 1.8070474863052368, time: [3324.76], best model: 1
Epoch: 28, train_loss: 1.8273354768753052, valid_loss: 1.7988008260726929, time: [138.31], best model: 1
Epoch: 29, train_loss: 1.818824052810669, valid_loss: 1.7887061834335327, time: [137.54], best model: 1
Epoch: 30, train_loss: 1.8118400573730469, valid_loss: 1.7767210006713867, time: [139.5], best model: 1
Epoch: 31, train_loss: 1.8040851354599, valid_loss: 1.7745782136917114, time: [136.29], best model: 1
Epoch: 32, train_loss: 1.7968120574951172, valid_loss: 1.7740334272384644, time: [7310.5], best model: 1
Epoch: 33, train_loss: 1.7912979125976562, valid_loss: 1.7655361890792847, time: [135.85], best model: 1
Epoch: 34, train_loss: 1.7815543413162231, valid_loss: 1.7557507753372192, time: [138.39], best model: 1
Epoch: 35, train_loss: 1.774856686592102, valid_loss: 1.756137728691101, time: [134.92], best model: 0
Epoch: 36, train_loss: 1.7698702812194824, valid_loss: 1.7477614879608154, time: [133.23], best model: 1
Epoch: 37, train_loss: 1.7628153562545776, valid_loss: 1.7361950874328613, time: [134.56], best model: 1
Epoch: 38, train_loss: 1.7541621923446655, valid_loss: 1.729054570198059, time: [1905.45], best model: 1
Epoch: 39, train_loss: 1.7482222318649292, valid_loss: 1.7336806058883667, time: [39.34], best model: 0
Epoch: 40, train_loss: 1.7412126064300537, valid_loss: 1.709841012954712, time: [38.94], best model: 1
Epoch: 41, train_loss: 1.735133171081543, valid_loss: 1.7037185430526733, time: [44.89], best model: 1
Epoch: 42, train_loss: 1.726444959640503, valid_loss: 1.7027018070220947, time: [40.74], best model: 1
Epoch: 43, train_loss: 1.7213398218154907, valid_loss: 1.698825478553772, time: [42.95], best model: 1
Epoch: 44, train_loss: 1.714425802230835, valid_loss: 1.6909235715866089, time: [38.77], best model: 1
Epoch: 45, train_loss: 1.7058067321777344, valid_loss: 1.687158226966858, time: [40.03], best model: 1
Epoch: 46, train_loss: 1.7007888555526733, valid_loss: 1.68197500705719, time: [39.24], best model: 1
Epoch: 47, train_loss: 1.6934939622879028, valid_loss: 1.6742314100265503, time: [33.06], best model: 1
Epoch: 48, train_loss: 1.6849020719528198, valid_loss: 1.668892741203308, time: [31.03], best model: 1
Epoch: 49, train_loss: 1.6781715154647827, valid_loss: 1.6572083234786987, time: [33.76], best model: 1
Epoch: 50, train_loss: 1.6739896535873413, valid_loss: 1.651283621788025, time: [34.87], best model: 1
Epoch: 51, train_loss: 1.666965126991272, valid_loss: 1.6484858989715576, time: [34.94], best model: 1
Epoch: 52, train_loss: 1.660904049873352, valid_loss: 1.650800108909607, time: [33.89], best model: 0
Epoch: 53, train_loss: 1.6543159484863281, valid_loss: 1.6342991590499878, time: [33.23], best model: 1
Epoch: 54, train_loss: 1.6480382680892944, valid_loss: 1.6302721500396729, time: [35.78], best model: 1
Epoch: 55, train_loss: 1.6418153047561646, valid_loss: 1.628371000289917, time: [32.65], best model: 1
Epoch: 56, train_loss: 1.6348620653152466, valid_loss: 1.6257929801940918, time: [31.91], best model: 1
Epoch: 57, train_loss: 1.6296573877334595, valid_loss: 1.6160759925842285, time: [36.82], best model: 1
Epoch: 58, train_loss: 1.623657464981079, valid_loss: 1.6073212623596191, time: [37.79], best model: 1
Epoch: 59, train_loss: 1.6164846420288086, valid_loss: 1.6089775562286377, time: [31.08], best model: 0
Epoch: 60, train_loss: 1.6083301305770874, valid_loss: 1.5939414501190186, time: [30.63], best model: 1
Epoch: 61, train_loss: 1.6058225631713867, valid_loss: 1.5956529378890991, time: [30.95], best model: 0
Epoch: 62, train_loss: 1.5975825786590576, valid_loss: 1.5900075435638428, time: [30.66], best model: 1
Epoch: 63, train_loss: 1.5934786796569824, valid_loss: 1.585873007774353, time: [30.61], best model: 1
Epoch: 64, train_loss: 1.5864944458007812, valid_loss: 1.577600121498108, time: [30.92], best model: 1
Epoch: 65, train_loss: 1.5827518701553345, valid_loss: 1.573468804359436, time: [30.39], best model: 1
Epoch: 66, train_loss: 1.5764727592468262, valid_loss: 1.5696889162063599, time: [30.49], best model: 1
Epoch: 67, train_loss: 1.5690594911575317, valid_loss: 1.5621062517166138, time: [30.98], best model: 1
Epoch: 68, train_loss: 1.5642848014831543, valid_loss: 1.5513675212860107, time: [30.55], best model: 1
Epoch: 69, train_loss: 1.5572394132614136, valid_loss: 1.54888117313385, time: [30.97], best model: 1
Epoch: 70, train_loss: 1.553794026374817, valid_loss: 1.542080283164978, time: [30.55], best model: 1
Epoch: 71, train_loss: 1.548277735710144, valid_loss: 1.5330867767333984, time: [31.16], best model: 1
Epoch: 72, train_loss: 1.541343331336975, valid_loss: 1.525951623916626, time: [30.65], best model: 1
Epoch: 73, train_loss: 1.5377624034881592, valid_loss: 1.527503490447998, time: [30.94], best model: 0
Epoch: 74, train_loss: 1.5320152044296265, valid_loss: 1.5198897123336792, time: [30.81], best model: 1
Epoch: 75, train_loss: 1.5268685817718506, valid_loss: 1.5202642679214478, time: [30.8], best model: 0
Epoch: 76, train_loss: 1.5213240385055542, valid_loss: 1.507562279701233, time: [30.68], best model: 1
Epoch: 77, train_loss: 1.5144621133804321, valid_loss: 1.511584758758545, time: [30.89], best model: 0
Epoch: 78, train_loss: 1.5108534097671509, valid_loss: 1.5043938159942627, time: [30.99], best model: 1
Epoch: 79, train_loss: 1.5056140422821045, valid_loss: 1.4981155395507812, time: [30.4], best model: 1
Epoch: 80, train_loss: 1.5006166696548462, valid_loss: 1.4881618022918701, time: [30.32], best model: 1
Epoch: 81, train_loss: 1.4957252740859985, valid_loss: 1.4897724390029907, time: [31.15], best model: 0
Epoch: 82, train_loss: 1.4890106916427612, valid_loss: 1.4853901863098145, time: [30.54], best model: 1
Epoch: 83, train_loss: 1.4848105907440186, valid_loss: 1.4808356761932373, time: [30.98], best model: 1
Epoch: 84, train_loss: 1.4806643724441528, valid_loss: 1.476228952407837, time: [30.8], best model: 1
Epoch: 85, train_loss: 1.4752238988876343, valid_loss: 1.4700771570205688, time: [31.65], best model: 1
Epoch: 86, train_loss: 1.470083475112915, valid_loss: 1.4773788452148438, time: [30.19], best model: 0
Epoch: 87, train_loss: 1.4639580249786377, valid_loss: 1.4611833095550537, time: [30.93], best model: 1
Epoch: 88, train_loss: 1.459662914276123, valid_loss: 1.4576860666275024, time: [30.65], best model: 1
Epoch: 89, train_loss: 1.455549716949463, valid_loss: 1.451650619506836, time: [30.82], best model: 1
Epoch: 90, train_loss: 1.4472389221191406, valid_loss: 1.4485194683074951, time: [30.72], best model: 1
Epoch: 91, train_loss: 1.445693016052246, valid_loss: 1.445624589920044, time: [31.18], best model: 1
Epoch: 92, train_loss: 1.4408347606658936, valid_loss: 1.4481706619262695, time: [30.92], best model: 0
Epoch: 93, train_loss: 1.4360393285751343, valid_loss: 1.4371814727783203, time: [31.68], best model: 1
Epoch: 94, train_loss: 1.4308059215545654, valid_loss: 1.4253543615341187, time: [30.75], best model: 1
Epoch: 95, train_loss: 1.4270224571228027, valid_loss: 1.424808144569397, time: [30.58], best model: 1
Epoch: 96, train_loss: 1.4223699569702148, valid_loss: 1.4201858043670654, time: [30.94], best model: 1
Epoch: 97, train_loss: 1.4174511432647705, valid_loss: 1.4155542850494385, time: [30.77], best model: 1
Epoch: 98, train_loss: 1.4136955738067627, valid_loss: 1.4113613367080688, time: [30.71], best model: 1
Epoch: 99, train_loss: 1.4092881679534912, valid_loss: 1.4095085859298706, time: [30.81], best model: 1
Epoch: 100, train_loss: 1.4071846008300781, valid_loss: 1.4099527597427368, time: [31.14], best model: 0
Epoch: 101, train_loss: 1.4022166728973389, valid_loss: 1.4002978801727295, time: [31.18], best model: 1
Epoch: 102, train_loss: 1.3993134498596191, valid_loss: 1.3995810747146606, time: [30.35], best model: 1
Epoch: 103, train_loss: 1.3961269855499268, valid_loss: 1.392053484916687, time: [31.45], best model: 1
Epoch: 104, train_loss: 1.3931591510772705, valid_loss: 1.389668583869934, time: [30.74], best model: 1
Epoch: 105, train_loss: 1.3899505138397217, valid_loss: 1.3870488405227661, time: [31.35], best model: 1
Epoch: 106, train_loss: 1.3863240480422974, valid_loss: 1.3854678869247437, time: [31.04], best model: 1
Epoch: 107, train_loss: 1.3828915357589722, valid_loss: 1.381652593612671, time: [30.91], best model: 1
Epoch: 108, train_loss: 1.3800973892211914, valid_loss: 1.3791464567184448, time: [31.27], best model: 1
Epoch: 109, train_loss: 1.3771629333496094, valid_loss: 1.3773037195205688, time: [30.23], best model: 1
Epoch: 110, train_loss: 1.3720195293426514, valid_loss: 1.3746345043182373, time: [31.04], best model: 1
Epoch: 111, train_loss: 1.3713781833648682, valid_loss: 1.3738402128219604, time: [31.57], best model: 1
Epoch: 112, train_loss: 1.3684719800949097, valid_loss: 1.3659801483154297, time: [30.94], best model: 1
Epoch: 113, train_loss: 1.3657801151275635, valid_loss: 1.3647855520248413, time: [30.93], best model: 1
Epoch: 114, train_loss: 1.36336350440979, valid_loss: 1.3616342544555664, time: [31.23], best model: 1
Epoch: 115, train_loss: 1.3609240055084229, valid_loss: 1.3611109256744385, time: [31.21], best model: 1
Epoch: 116, train_loss: 1.3586584329605103, valid_loss: 1.3622281551361084, time: [31.28], best model: 0
Epoch: 117, train_loss: 1.3563507795333862, valid_loss: 1.3573670387268066, time: [31.87], best model: 1
Epoch: 118, train_loss: 1.354079246520996, valid_loss: 1.3613777160644531, time: [34.65], best model: 0
Epoch: 119, train_loss: 1.3522883653640747, valid_loss: 1.355682134628296, time: [32.48], best model: 1
Epoch: 120, train_loss: 1.3500847816467285, valid_loss: 1.3522762060165405, time: [31.21], best model: 1
Epoch: 121, train_loss: 1.3483022451400757, valid_loss: 1.3503737449645996, time: [31.61], best model: 1
Epoch: 122, train_loss: 1.3462435007095337, valid_loss: 1.3512721061706543, time: [31.69], best model: 0
Epoch: 123, train_loss: 1.344562292098999, valid_loss: 1.3531968593597412, time: [31.31], best model: 0
Epoch: 124, train_loss: 1.3426811695098877, valid_loss: 1.3475022315979004, time: [31.32], best model: 1
Epoch: 125, train_loss: 1.3427950143814087, valid_loss: 1.3511921167373657, time: [31.5], best model: 0
Epoch: 126, train_loss: 1.3363434076309204, valid_loss: 1.3441506624221802, time: [31.73], best model: 1
Epoch: 127, train_loss: 1.33872652053833, valid_loss: 1.347590446472168, time: [31.75], best model: 0
Epoch: 128, train_loss: 1.3377567529678345, valid_loss: 1.349329948425293, time: [31.7], best model: 0
Epoch: 129, train_loss: 1.3366378545761108, valid_loss: 1.344279408454895, time: [31.65], best model: 0
Epoch: 130, train_loss: 1.335355281829834, valid_loss: 1.342250108718872, time: [32.09], best model: 1
Epoch: 131, train_loss: 1.3342769145965576, valid_loss: 1.3453559875488281, time: [32.48], best model: 0
Epoch: 132, train_loss: 1.3330893516540527, valid_loss: 1.3365793228149414, time: [32.14], best model: 1
Epoch: 133, train_loss: 1.3317103385925293, valid_loss: 1.3428915739059448, time: [32.11], best model: 0
Epoch: 134, train_loss: 1.3303896188735962, valid_loss: 1.3395177125930786, time: [31.61], best model: 0
Epoch: 135, train_loss: 1.3289129734039307, valid_loss: 1.342153549194336, time: [32.02], best model: 0
Epoch: 136, train_loss: 1.3274390697479248, valid_loss: 1.3414753675460815, time: [31.75], best model: 0
Epoch: 137, train_loss: 1.3258661031723022, valid_loss: 1.3382436037063599, time: [31.89], best model: 0
Epoch: 138, train_loss: 1.3241291046142578, valid_loss: 1.3372710943222046, time: [31.31], best model: 0
Epoch: 139, train_loss: 1.3202394247055054, valid_loss: 1.336422324180603, time: [31.88], best model: 1
Epoch: 140, train_loss: 1.3208972215652466, valid_loss: 1.3397738933563232, time: [31.58], best model: 0
Epoch: 141, train_loss: 1.3226269483566284, valid_loss: 1.3492146730422974, time: [32.42], best model: 0
Epoch: 142, train_loss: 1.320465326309204, valid_loss: 1.3456852436065674, time: [31.53], best model: 0
Epoch: 143, train_loss: 1.3193970918655396, valid_loss: 1.3488190174102783, time: [31.64], best model: 0
Epoch: 144, train_loss: 1.3179209232330322, valid_loss: 1.3415064811706543, time: [31.8], best model: 0
Epoch: 145, train_loss: 1.316534399986267, valid_loss: 1.3434771299362183, time: [32.05], best model: 0
Epoch: 146, train_loss: 1.315352439880371, valid_loss: 1.350695013999939, time: [32.84], best model: 0
Epoch: 147, train_loss: 1.3141324520111084, valid_loss: 1.3492201566696167, time: [31.16], best model: 0
Epoch: 148, train_loss: 1.3130512237548828, valid_loss: 1.3428374528884888, time: [33.15], best model: 0
Early Stopped at Epoch: 149
btach accuracy 1.0 , batch id 0
btach accuracy 1.0 , batch id 1
btach accuracy 1.0 , batch id 2
btach accuracy 1.0 , batch id 3
btach accuracy 1.0 , batch id 4
btach accuracy 1.0 , batch id 5
btach accuracy 1.0 , batch id 6
btach accuracy 1.0 , batch id 7
btach accuracy 1.0 , batch id 8
btach accuracy 1.0 , batch id 9
Tested #: 20, loss_l1: [1.2845483] ,  time: [0.94531703]
btach accuracy 0.5 , batch id 10
btach accuracy 1.0 , batch id 11
btach accuracy 1.0 , batch id 12
btach accuracy 1.0 , batch id 13
btach accuracy 1.0 , batch id 14
btach accuracy 1.0 , batch id 15
btach accuracy 1.0 , batch id 16
btach accuracy 1.0 , batch id 17
btach accuracy 1.0 , batch id 18
btach accuracy 1.0 , batch id 19
Tested #: 40, loss_l1: [1.2853161] ,  time: [0.88360691]
Tested: Cross Entropy losses: [1.3042345 1.2878761 1.2892272 1.2875957 1.2837086 1.2923998 1.284622
 1.3161588 1.3020945 1.2845483 1.6702468 1.2890985 1.2943537 1.2898245
 1.2912246 1.2934501 1.2887573 1.2964195 1.2809403 1.2853161] 
accuracy_score 0.975
confusion_matrix [[3 0 0 1 0 0 0 0]
 [0 8 0 0 0 0 0 0]
 [0 0 4 0 0 0 0 0]
 [0 0 0 9 0 0 0 0]
 [0 0 0 0 5 0 0 0]
 [0 0 0 0 0 4 0 0]
 [0 0 0 0 0 0 3 0]
 [0 0 0 0 0 0 0 3]]
Confusion matrix, without normalization
[[3 0 0 1 0 0 0 0]
 [0 8 0 0 0 0 0 0]
 [0 0 4 0 0 0 0 0]
 [0 0 0 9 0 0 0 0]
 [0 0 0 0 5 0 0 0]
 [0 0 0 0 0 4 0 0]
 [0 0 0 0 0 0 3 0]
 [0 0 0 0 0 0 0 3]]

Process finished with exit code 0
